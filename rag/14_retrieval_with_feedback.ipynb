{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System with Feedback Loop: Enhancing Retrieval and Response Quality\n",
    "## Overview\n",
    "This system implements a RAG approach with an integrated feedback loop. It aims to improve the quality and relevance of responses over time by incorporating user feedback and dynamically adjusting the retrieval process.\n",
    "## Motivation\n",
    "Tranditional RAG systems can sometimes produce inconsistent or irrelevant responses due to limitations in the retrieval process or the underlying knowledge base. By implementing a feedback loop, we can:\n",
    "- Continuously improve the quality of retrieved documents\n",
    "- Enhance the relevance of generated responses\n",
    "- Adapt the system to user preferences and needs over time\n",
    "## Key Components\n",
    "- PDF Content Extraction: Extracts text from PDF documents\n",
    "- Vectorstore: Stores and indexes document embeddings for efficient retrieval\n",
    "- Retriever: Fetches relevant document based on user queries\n",
    "- LLM: Generates responses using retrieved documents\n",
    "- Feedback Collection: Gathers user feedback on response quality and relevance\n",
    "- Feedback Storage: Persists user feedback for future use\n",
    "- Relevance Score Adjustment: Modifies document relevance based on feedback\n",
    "- Index Fine-tuning: Periodically updates the vectorstore using accumulated feedback\n",
    "## Benefits of this Approach\n",
    "1. Continuous Improvement: The system learns from each interaction, gradually enhancing its performance\n",
    "2. Personalization: By incorporating user feedback, the system can adapt to individual or group preferences over time\n",
    "3. Increased Relevance: The feedback loop helps prioritize more relevant documents in future retrievals\n",
    "4. Quality Control: Low-quality or irrelevant responses are less likely to be repeated as the system evolves\n",
    "5. Adaptability: The system can adjust to changes in user needs or document contents over time\n",
    "## Conclusion\n",
    "This RAG system with a feedback loop represents a significant advancement over traditional RAG implementations. By continuously learning from user interactions, it offers a more dynamic, adaptive, and user-centric approach to information retrieval and response generation. This system is particularly valuable in domains where information accuracy and relevance are critical, and where user needs may evolve over time.\n",
    "\n",
    "While the implementation adds complexity compared to a basic RAG system, the benefits in terms of response quality and user satisfaction make it a worthwhile investment for applications requiring high-quality, context-aware information retrieval and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
    "load_dotenv()\n",
    "openai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_ID\")\n",
    "openai_api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    api_version=\"2024-10-01-preview\",\n",
    "    azure_endpoint=f\"{openai_endpoint}openai/deployments/{openai_deployment}/chat/completions?api-version=2024-10-01-preview\",\n",
    "    temperature=0,\n",
    "    logprobs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from helper_functions import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Any, Tuple\n",
    "path = \"./data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import read_pdf_to_string, encode_from_string\n",
    "openai_embedding = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_ID\")\n",
    "\n",
    "from langchain_openai.embeddings.azure import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=openai_embedding,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    chunk_size=16\n",
    ")\n",
    "content = read_pdf_to_string(path)\n",
    "vectorstore = encode_from_string(content, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevance\": int(relevance),\n",
    "        \"quality\": int(quality),\n",
    "        \"comments\": comments\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def store_feedback(feedback):\n",
    "    with open(\"./data/feedback_data.json\", \"a\") as f:\n",
    "        json.dump(feedback, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feedback_data():\n",
    "    feedback_data = []\n",
    "    try:\n",
    "        with open(\"./data/feedback_data.json\", \"r\") as f:\n",
    "            for line in f:\n",
    "                feedback_data.append(json.loads(line.strip()))\n",
    "    except FileNotFoundError:\n",
    "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
    "    return feedback_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "class Response(BaseModel):\n",
    "    answer: str = Field(..., title=\"The answer to the question. The options can be only 'Yes' or 'No'\")\n",
    "\n",
    "def adjust_relevance_scores(query: str, docs: List[Any], feedback_data: List[Dict[str, Any]]) -> List[Any]:\n",
    "    # Create a prompt template for relevance checking\n",
    "    relevance_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"feedback_query\", \"doc_content\", \"feedback_response\"],\n",
    "        template=\"\"\"\n",
    "        Determine if the following feedback response is relevant to the current query and document content.\n",
    "        You are also provided with the Feedback original query that was used to generate the feedback response.\n",
    "        Current query: {query}\n",
    "        Feedback query: {feedback_query}\n",
    "        Document content: {doc_content}\n",
    "        Feedback response: {feedback_response}\n",
    "        \n",
    "        Is this feedback relevant? Respond with only 'Yes' or 'No'.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Create an LLMChain for relevance checking\n",
    "    relevance_chain = relevance_prompt | llm.with_structured_output(Response)\n",
    "\n",
    "    for doc in docs:\n",
    "        relevant_feedback = []\n",
    "        \n",
    "        for feedback in feedback_data:\n",
    "            # Use LLM to check relevance\n",
    "            input_data = {\n",
    "                \"query\": query,\n",
    "                \"feedback_query\": feedback['query'],\n",
    "                \"doc_content\": doc.page_content[:1000],\n",
    "                \"feedback_response\": feedback['response']\n",
    "            }\n",
    "            result = relevance_chain.invoke(input_data).answer\n",
    "            \n",
    "            if result == 'yes':\n",
    "                relevant_feedback.append(feedback)\n",
    "        \n",
    "        # Adjust the relevance score based on feedback\n",
    "        if relevant_feedback:\n",
    "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
    "            doc.metadata['relevance_score'] *= (avg_relevance / 3)  # Assuming a 1-5 scale, 3 is neutral\n",
    "    \n",
    "    # Re-rank documents based on adjusted scores\n",
    "    return sorted(docs, key=lambda x: x.metadata['relevance_score'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_index(feedback_data: List[Dict[str, Any]], texts: List[str]) -> Any:\n",
    "    # Filter high-quality responses\n",
    "    good_responses = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
    "    \n",
    "    # Extract queries and responses, and create new documents\n",
    "    additional_texts = []\n",
    "    for f in good_responses:\n",
    "        combined_text = f['query'] + \" \" + f['response']\n",
    "        additional_texts.append(combined_text)\n",
    "\n",
    "    # make the list a string\n",
    "    additional_texts = \" \".join(additional_texts)\n",
    "    \n",
    "    # Create a new index with original and high-quality texts\n",
    "    all_texts = texts + additional_texts\n",
    "    new_vectorstore = encode_from_string(all_texts, embedding=embeddings)\n",
    "    \n",
    "    return new_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "\n",
    "# Get response from RAG system\n",
    "response = qa_chain.invoke(query)[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = 5\n",
    "quality = 5\n",
    "\n",
    "# Collect feedback\n",
    "feedback = get_user_feedback(query, response, relevance, quality)\n",
    "\n",
    "# Store feedback\n",
    "store_feedback(feedback)\n",
    "\n",
    "# Adjust relevance scores for future retrievals\n",
    "docs = retriever.invoke(query)\n",
    "adjusted_docs = adjust_relevance_scores(query, docs, load_feedback_data())\n",
    "\n",
    "# Update the retriever with adjusted docs\n",
    "retriever.search_kwargs['k'] = len(adjusted_docs)\n",
    "retriever.search_kwargs['docs'] = adjusted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorstore = fine_tune_index(load_feedback_data(), content)\n",
    "retriever = new_vectorstore.as_retriever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
