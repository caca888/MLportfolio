{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Compression in Document Retrieval\n",
    "## Overview\n",
    "This code demonstrates the implementation of contextual compression in a document retrieval system using LangChain and LLM. The technique aims to improve the reelvance and conciseness of retrieved information by compressing and extracting the most pertinent parts of documents in the context of a given query.\n",
    "## Motivation\n",
    "Traditional document retrieval systems often return entire chunks or documents, which may contain irrelevant information. Contextual compression addresses this by interlligently extracting and compressing only the most relevant parts of retrieved documents, leading to more focused and efficient information retrieval.\n",
    "## Key Components\n",
    "1. Vector strore creation from a PDF document\n",
    "2. Base retriever setup\n",
    "3. LLM-based contextual compressor\n",
    "4. Contextual compression retriever\n",
    "5. Question-answering chain integrating the compressed retriever\n",
    "## Method Detail\n",
    "### Document Preprocessing and Vector Store Creation\n",
    "PDF is processed and encoded into a vector store using a `encode_pdf` function\n",
    "### Retriever and Compressor Setup\n",
    "- A based retriever is created from the vector store\n",
    "- An LLM-based contextual compressor is initialized \n",
    "### Contextual Compression Retriever\n",
    "- The base retriever and compressor are combined into a ContextualCompressionRetriever\n",
    "- The retriever first fetches documents using the base retriever, then applies the compressor to extract the most relevant information\n",
    "### Question-Answering Chain\n",
    "- RetrivalQA chain is created, integrating the compression retriever\n",
    "- This chain uses the compressed and extracted information to generate answers to queries\n",
    "## Benefits of this Approach\n",
    "1. Improved relevance: The system returns only the most pertinent information to the query\n",
    "2. Increased efficiency: By compressing and extracting relevant parts, it reduces the amount of text the LLM needs to process\n",
    "3. Enhanced context understanding: The LLM-based compressor can understand the context of the query and extract information accordingly\n",
    "4. Flexibility: The system can be deasily adapted to different types of documents and queries\n",
    "## Conclusion\n",
    "Contextual compression in document retrieval offers a powerful way to enhance the quality and efficiency of information retrieval systems. By intelligently extracting an dcompressing relevant information, it provides more focused and context-aware responses to queries. This approach has potential applications in various fields requiring efficient and accurate information retrieval from large document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
    "load_dotenv()\n",
    "openai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_ID\")\n",
    "openai_api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    api_version=\"2024-10-01-preview\",\n",
    "    azure_endpoint=f\"{openai_endpoint}openai/deployments/{openai_deployment}/chat/completions?api-version=2024-10-01-preview\",\n",
    "    temperature=0,\n",
    "    logprobs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedding = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_ID\")\n",
    "\n",
    "from langchain_openai.embeddings.azure import AzureOpenAIEmbeddings\n",
    "from helper_functions import PyPDFLoader, RecursiveCharacterTextSplitter, replace_t_with_space, FAISS\n",
    "\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        deployment=openai_embedding,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        chunk_size=16\n",
    "    )\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorestore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "retriever = vectorestore.as_retriever()\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main topic of the document is climate action, focusing on both global and local efforts to address climate change. It discusses international collaboration through frameworks like the United Nations Framework Convention on Climate Change (UNFCCC) and the Paris Agreement, as well as national strategies such as carbon pricing mechanisms.\n",
      "Source documents: [Document(metadata={'source': './data/Understanding_Climate_Change.pdf', 'page': 9}, page_content='Chapter 6: Global and Local Climate Action  \\nInternational Collaboration  \\nUnited Nations Framework Convention on Climate Change (UNFCCC)  \\nThe UNFCCC is an international treaty aimed at addressing climate change. It provides a \\nframework for negotiating specific protocols and agreements, such as the Kyoto Protocol and \\nthe Paris Agreement. Global cooperation under the UNFCCC is crucial for coor dinated \\nclimate action.  \\nParis Agreement  \\nThe Paris Agreement, adopted in 2015, aims to limit global warming to well below 2 degrees \\nCelsius above pre -industrial levels, with efforts to limit the increase to 1.5 degrees Celsius. \\nCountries submit nationally determined contributions (NDCs) outlining  their climate action \\nplans and targets.  \\nNational Strategies  \\nCarbon Pricing  \\nCarbon pricing mechanisms, such as carbon taxes and cap -and-trade systems, incentivize \\nemission reductions by assigning a cost to carbon emissions. These policies encourage')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main topic of the document?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "print(result[\"result\"])\n",
    "print(\"Source documents:\", result[\"source_documents\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
