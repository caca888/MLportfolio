{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Indices in Document Retrieval\n",
    "## Overview\n",
    "This code implements a Hierarchical Indexing system for document retrieval, utilizing two levels of encoding: document-level summaries and detailed chunks. This approach aims to improve the efficiency and relevance of information retrieval by fist identifying relevant documnent sections through summaries, then drilling down to specific details within those sections.\n",
    "## Motivation\n",
    "Traditional flat indexing methods can struggle with large documents or corpus, potentially missing context or returning irrelevant information. Hierarchical indexing addresses this by creating a two-tier search system, allowing for more efficient and context-aware retrieval.\n",
    "## Key Components\n",
    "- PDF processing and text chunking\n",
    "- Asynchronous document summarization using LLM\n",
    "- Vector store creation for both summaries and detailed chunks using FAISS and embeddings\n",
    "- Custom hierarchical retrieval function\n",
    "## Benefits of this Approach\n",
    "- Improved Retrieval Efficiency: By first searching summaries, the system can quickly identify relevant document sections without processing all detailed chunks\n",
    "- Better Context Preservation: The hierarchical approach helps maintain the broader context of retrieved information\n",
    "- Scalability: This method is particularly beneficial for large documents or corpus, where flat searching might be inefficient or miss important context\n",
    "- Flexibility: The system allows for adjusting the number of summaries and chunks retrieved, enabling fine-tuning for different use cases\n",
    "## Conclusion\n",
    "Hierarchical indexing represents a sophisticated approach to document retrieval, particularly suitable for large or complex document sets. By leveraging both high-level summaries and detailed chunks, it offers a balance between broad context understanding and specifc information retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
    "load_dotenv()\n",
    "openai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_ID\")\n",
    "openai_api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    api_version=\"2024-10-01-preview\",\n",
    "    azure_endpoint=f\"{openai_endpoint}openai/deployments/{openai_deployment}/chat/completions?api-version=2024-10-01-preview\",\n",
    "    temperature=0,\n",
    "    logprobs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize.chain import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "path = \"./data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'retry_with_exponentail_backoff' from 'helper_functions' (c:\\Users\\uidp8109\\OneDrive - Continental AG\\projects\\AdvancedRAG\\helper_functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader, RecursiveCharacterTextSplitter, retry_with_exponentail_backoff, FAISS\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'retry_with_exponentail_backoff' from 'helper_functions' (c:\\Users\\uidp8109\\OneDrive - Continental AG\\projects\\AdvancedRAG\\helper_functions.py)"
     ]
    }
   ],
   "source": [
    "from helper_functions import PyPDFLoader, RecursiveCharacterTextSplitter, retry_with_exponentail_backoff, FAISS\n",
    "import asyncio\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "openai_embedding = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_ID\")\n",
    "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
    "    \"\"\"\n",
    "    Asynchronously encodes a PDF book into a hierarchical vector store using embeddings.\n",
    "    Includes rate limit handling with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing two FAISS vector stores:\n",
    "        1. Document-level summaries\n",
    "        2. Detailed chunks\n",
    "    \"\"\"\n",
    "    if not is_string:\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = await asyncio.to_thread(loader.load)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, is_separator_regex=False,\n",
    "        )\n",
    "        documents = text_splitter.create_documents([path])\n",
    "\n",
    "    summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")\n",
    "\n",
    "    async def summarize_document(document):\n",
    "        \"\"\"\n",
    "        Summarizes a single document with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            doc: The document to be summarized.\n",
    "            \n",
    "        Returns:\n",
    "            A summarized Document object.\n",
    "        \"\"\"\n",
    "        summary_output = await retry_with_exponentail_backoff(summary_chain.ainvoke([document]))\n",
    "        summary = summary_output[\"output_text\"]\n",
    "        return Document(\n",
    "            page_content=summary,\n",
    "            metadata={\"source\": path, \"page\": document.metadata[\"page\"], \"summary\": True},\n",
    "        )\n",
    "    \n",
    "    batch_size = 5\n",
    "    summaries = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        batch_summaries = await asyncio.gather(*[summarize_document(doc) for doc in batch])\n",
    "        summaries.extend(batch_summaries)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\"chunk_id\": i, \"summary\": False, \"page\": int(chunk.metadata.get(\"page\", 0))})\n",
    "\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        deployment=openai_embedding,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        chunk_size=16\n",
    "    )\n",
    "\n",
    "    async def create_vectorstor(docs):\n",
    "        \"\"\"\n",
    "        Creates a vector store from a list of documents with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            docs: The list of documents to be embedded.\n",
    "            \n",
    "        Returns:\n",
    "            A FAISS vector store containing the embedded documents.\n",
    "        \"\"\"\n",
    "        return await retry_with_exponentail_backoff(\n",
    "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
    "        )\n",
    "    \n",
    "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "        create_vectorstor(summaries),\n",
    "        create_vectorstor(detailed_chunks),\n",
    "    )\n",
    "\n",
    "    return summary_vectorstore, detailed_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./vector_stores/summary_store\") and os.path.exists(\"./vector_stores/detailed_store\"):\n",
    "   embeddings = AzureOpenAIEmbeddings(\n",
    "        deployment=openai_embedding,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        chunk_size=16\n",
    "    )\n",
    "   summary_store = FAISS.load_local(\"./vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "   detailed_store = FAISS.load_local(\"./vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "else:\n",
    "    summary_store, detailed_store = await encode_pdf_hierarchical(path)\n",
    "    summary_store.save_local(\"./vector_stores/summary_store\")\n",
    "    detailed_store.save_local(\"./vector_stores/detailed_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Performs a hierarchical retrieval using the query.\n",
    "\n",
    "    Args:\n",
    "        query: The search query.\n",
    "        summary_vectorstore: The vector store containing document summaries.\n",
    "        detailed_vectorstore: The vector store containing detailed chunks.\n",
    "        k_summaries: The number of top summaries to retrieve.\n",
    "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
    "\n",
    "    Returns:\n",
    "        A list of relevant detailed chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve top summaries\n",
    "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    for summary in top_summaries:\n",
    "        # For each summary, retrieve relevant detailed chunks\n",
    "        page_number = summary.metadata[\"page\"]\n",
    "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "        page_chunks = detailed_vectorstore.similarity_search(\n",
    "            query, \n",
    "            k=k_chunks, \n",
    "            filter=page_filter\n",
    "        )\n",
    "        relevant_chunks.extend(page_chunks)\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
    "\n",
    "# Print results\n",
    "for chunk in results:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
